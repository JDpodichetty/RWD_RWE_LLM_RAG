{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Vector Store Creation (Docker with Chunking) ---\n",
      "Initializing embedding model: 'answerdotai/ModernBERT-base'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name answerdotai/ModernBERT-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Qdrant at: http://localhost:8000\n",
      "Collection 'my_text_collection' not found. Creating new collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6876/1379417601.py:54: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'my_text_collection' created successfully.\n",
      "Scanning and chunking files in 'Documents_Cleaned_Editable'...\n",
      "  - Chunked 'cleaned_Pharmacoepidemiology and Drug - 2022 - Girman - Real‐world data  Assessing electronic health records and medical claims.txt' into 22 pieces.\n",
      "  - Chunked 'cleaned_ICH reflection paper on pursuing opportunities for RWD.txt' into 37 pieces.\n",
      "  - Chunked 'cleaned_FDA’s Real-World Evidence Program Framework.txt' into 108 pieces.\n",
      "  - Chunked 'cleaned_ICMRA Statement on International Collaboration (RWE).txt' into 8 pieces.\n",
      "\n",
      "Generating embeddings for 175 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [02:59<00:00, 29.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 175 chunks to Qdrant in batches of 64...\n",
      "  - Uploaded batch 1/3\n",
      "  - Uploaded batch 2/3\n",
      "  - Uploaded batch 3/3\n",
      "\n",
      "Successfully uploaded 175 chunks to Qdrant collection 'my_text_collection'.\n",
      "\n",
      "--- ✅ Vector Store Creation Finished! ---\n",
      "Your vector data is stored in the Qdrant container.\n",
      "Persistent data is saved in the 'qdrant_storage' folder.\n"
     ]
    }
   ],
   "source": [
    "# vector_store_creator.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# 1. DEFINE FOLDER WITH YOUR TEXT FILES\n",
    "#    The script will scan this folder for any .txt files.\n",
    "TEXT_FILES_FOLDER = \"Documents_Cleaned_Editable\"\n",
    "\n",
    "# 2. DEFINE THE EMBEDDING MODEL\n",
    "#    This model is specialized for biomedical and scientific text.\n",
    "MODEL_NAME = \"NeuML/bioclinical-modernbert-base-embeddings\"\n",
    "\n",
    "# 3. DEFINE QDRANT CONFIGURATION\n",
    "#    This script connects to a Qdrant instance running in Docker.\n",
    "QDRANT_URL = \"http://localhost:8000\"\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "\n",
    "# 4. DEFINE TEXT CHUNKING PARAMETERS\n",
    "CHUNK_SIZE = 1024  # Max characters per chunk\n",
    "CHUNK_OVERLAP = 200 # Characters to overlap between chunks\n",
    "\n",
    "# 5. DEFINE UPLOAD BATCH SIZE\n",
    "#    Process and upload documents in batches to avoid connection timeouts.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def setup_vector_store():\n",
    "    \"\"\"\n",
    "    Sets up the Qdrant vector store and the embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing the Qdrant client and the embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing embedding model: '{MODEL_NAME}'...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "    print(f\"Connecting to Qdrant at: {QDRANT_URL}\")\n",
    "    client = QdrantClient(url=QDRANT_URL)\n",
    "    \n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    try:\n",
    "        client.get_collection(collection_name=COLLECTION_NAME)\n",
    "        print(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Collection '{COLLECTION_NAME}' not found. Creating new collection...\")\n",
    "        client.recreate_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=embedding_dim,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "        \n",
    "    return client, model\n",
    "\n",
    "\n",
    "def create_and_store_embeddings(client: QdrantClient, model: SentenceTransformer, folder_path: str):\n",
    "    \"\"\"\n",
    "    Reads files, chunks their content, generates embeddings, and upserts them into Qdrant in batches.\n",
    "\n",
    "    Args:\n",
    "        client: The Qdrant client instance.\n",
    "        model: The Sentence Transformer model instance.\n",
    "        folder_path: The path to the folder containing .txt files.\n",
    "    \"\"\"\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    all_metadata = []\n",
    "\n",
    "    print(f\"Scanning and chunking files in '{folder_path}'...\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                if content.strip():\n",
    "                    # Split the document content into chunks\n",
    "                    chunks = text_splitter.split_text(content)\n",
    "                    for chunk in chunks:\n",
    "                        all_chunks.append(chunk)\n",
    "                        all_metadata.append({\"file_path\": file_path})\n",
    "                    print(f\"  - Chunked '{filename}' into {len(chunks)} pieces.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading or chunking {filename}: {e}\")\n",
    "\n",
    "    if not all_chunks:\n",
    "        print(\"No text content to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "    \n",
    "    # Generate embeddings for all chunks in a single batch for efficiency\n",
    "    vectors = model.encode(all_chunks, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"Uploading {len(all_chunks)} chunks to Qdrant in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Upload to Qdrant in smaller batches\n",
    "    for i in range(0, len(all_chunks), BATCH_SIZE):\n",
    "        batch_end = i + BATCH_SIZE\n",
    "        chunk_batch = all_chunks[i:batch_end]\n",
    "        vector_batch = vectors[i:batch_end]\n",
    "        metadata_batch = all_metadata[i:batch_end]\n",
    "\n",
    "        points_to_upsert = []\n",
    "        for j, vector in enumerate(vector_batch):\n",
    "            point = models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector.tolist(),\n",
    "                payload={\n",
    "                    \"file_path\": metadata_batch[j][\"file_path\"],\n",
    "                    \"content\": chunk_batch[j]\n",
    "                }\n",
    "            )\n",
    "            points_to_upsert.append(point)\n",
    "\n",
    "        # Upsert the batch to the collection\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points_to_upsert,\n",
    "            wait=True\n",
    "        )\n",
    "        print(f\"  - Uploaded batch {i//BATCH_SIZE + 1}/{(len(all_chunks) - 1)//BATCH_SIZE + 1}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully uploaded {len(all_chunks)} chunks to Qdrant collection '{COLLECTION_NAME}'.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the vector store creation pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Vector Store Creation (Docker with Chunking) ---\")\n",
    "\n",
    "    if not os.path.isdir(TEXT_FILES_FOLDER):\n",
    "        print(f\"❌ Error: Input folder '{TEXT_FILES_FOLDER}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not any(f.endswith('.txt') for f in os.listdir(TEXT_FILES_FOLDER)):\n",
    "        print(f\"⚠️ Warning: No .txt files found in '{TEXT_FILES_FOLDER}'.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    qdrant_client, embedding_model = setup_vector_store()\n",
    "    create_and_store_embeddings(qdrant_client, embedding_model, TEXT_FILES_FOLDER)\n",
    "    \n",
    "    print(\"\\n--- ✅ Vector Store Creation Finished! ---\")\n",
    "    print(f\"Your vector data is stored in the Qdrant container.\")\n",
    "    print(f\"Persistent data is saved in the 'qdrant_storage' folder.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Installation ---\n",
    "    # pip install qdrant-client sentence-transformers langchain\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579393a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
